{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = np.load(\"minist/x_train.npy\")\n",
    "y_train = np.load(\"minist/y_train.npy\")\n",
    "x_test = np.load(\"minist/x_test.npy\")\n",
    "y_test = np.load(\"minist/y_test.npy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x2921474f948>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.imshow(x_train[15])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(784, 60000) (784, 10000)\n"
     ]
    }
   ],
   "source": [
    "x_train_fallten = x_train.reshape(x_train.shape[0],-1).T\n",
    "x_test_fallten = x_test.reshape(x_test.shape[0],-1).T\n",
    "print(x_train_fallten.shape,x_test_fallten.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_x = x_train_fallten / 255\n",
    "test_x = x_test_fallten / 255\n",
    "y_test = y_test\n",
    "y_train= y_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(z):\n",
    "    return 1/(1+np.exp(-z))# 求e次方"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5"
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sigmoid(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialize_with_zeros(dim):\n",
    "    \"\"\"\n",
    "    This function creates a vector of zeros of shape (dim, 1) for w and initializes b to 0.\n",
    "    \n",
    "    Argument:\n",
    "    dim -- size of the w vector we want (or number of parameters in this case)\n",
    "    \n",
    "    Returns:\n",
    "    w -- initialized vector of shape (dim, 1)\n",
    "    b -- initialized scalar (corresponds to the bias)\n",
    "    \"\"\"\n",
    "    \n",
    "    ### START CODE HERE ### (≈ 1 line of code)\n",
    "    w = np.zeros(shape=(dim, 10))\n",
    "    b = 0\n",
    "    ### END CODE HERE ###\n",
    "\n",
    "    assert(w.shape == (dim, 10))\n",
    "    assert(isinstance(b, float) or isinstance(b, int))\n",
    "    \n",
    "    return w, b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def propagate(w, b, X, Y):\n",
    "    # 正向计算cost\n",
    "    m = X.shape[1] # 获取数据集的大小\n",
    "    A = sigmoid(np.dot(w.T,X) + b)\n",
    "    print(A)\n",
    "    # cost = -Y*np.log(A)-(1-Y)*np.log(1-A)   这是错误的计算方法，因为此时的A，X中都包含了数据集的所有结果，所以，这样运行得到的，是整个\n",
    "    # cost的值，而不是单个的，所以，对于整合cost的计算，要按照下面的公式\n",
    "    cost = -(1/m)*np.sum(Y*np.log(A)+ (1-Y)*np.log(1-A))\n",
    "    \n",
    "    # 反向计算梯度\n",
    "    dw = (1 / m) * np.dot(X, (A - Y).T)\n",
    "    db = (1 / m) * np.sum(A - Y)\n",
    "    \n",
    "    #整合梯度函数\n",
    "    grads = {\n",
    "        \"dw\":dw,\n",
    "        \"db\":db\n",
    "    }\n",
    "    cost = np.squeeze(cost)\n",
    "    \n",
    "    return cost,grads"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w,b = initialize_with_zeros(train_x.shape[0])\n",
    "cost ,grads = propagate(w,b,train_x,y_train)\n",
    "print(\"一次迭代的花费为\",cost)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def optimize(w, b, X, Y, num_iterations, learning_rate, print_cost = False):\n",
    "    \"\"\"\n",
    "    This function optimizes w and b by running a gradient descent algorithm\n",
    "    \n",
    "    Arguments:\n",
    "    w -- weights, a numpy array of size (num_px * num_px * 3, 1)\n",
    "    b -- bias, a scalar\n",
    "    X -- data of shape (num_px * num_px * 3, number of examples)\n",
    "    Y -- true \"label\" vector (containing 0 if non-cat, 1 if cat), of shape (1, number of examples)\n",
    "    num_iterations -- number of iterations of the optimization loop\n",
    "    learning_rate -- learning rate of the gradient descent update rule\n",
    "    print_cost -- True to print the loss every 100 steps\n",
    "    \n",
    "    Returns:\n",
    "    params -- dictionary containing the weights w and bias b\n",
    "    grads -- dictionary containing the gradients of the weights and bias with respect to the cost function\n",
    "    costs -- list of all the costs computed during the optimization, this will be used to plot the learning curve.\n",
    "    \n",
    "    Tips:\n",
    "    You basically need to write down two steps and iterate through them:\n",
    "        1) Calculate the cost and the gradient for the current parameters. Use propagate().\n",
    "        2) Update the parameters using gradient descent rule for w and b.\n",
    "    \"\"\"\n",
    "    costs = []\n",
    "    for i in range(num_iterations):\n",
    "        cost,grads = propagate(w,b,X,Y)\n",
    "        dw = grads[\"dw\"]\n",
    "        db = grads[\"db\"]\n",
    "        \n",
    "        # 计算新的wb\n",
    "        w = w - learning_rate*dw\n",
    "        b = b - learning_rate*db\n",
    "        \n",
    "        # 每一百次记录一下cost\n",
    "        if i % 100 == 0:\n",
    "            costs.append(cost)\n",
    "        \n",
    "        # 每一百输出一次\n",
    "        if print_cost and i % 100 == 0:\n",
    "            print(\"Cost after iteration %i: %f\" % (i, cost))\n",
    "    params = {\n",
    "        \"w\":w,\n",
    "        \"b\":b\n",
    "    }\n",
    "    \n",
    "    grads = {\n",
    "        \"dw\":dw,\n",
    "        \"db\":db\n",
    "    }\n",
    "    return params,grads,costs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(w, b, X):\n",
    "    '''\n",
    "    Predict whether the label is 0 or 1 using learned logistic regression parameters (w, b)\n",
    "    \n",
    "    Arguments:\n",
    "    w -- weights, a numpy array of size (num_px * num_px * 3, 1)\n",
    "    b -- bias, a scalar\n",
    "    X -- data of size (num_px * num_px * 3, number of examples)\n",
    "    \n",
    "    Returns:\n",
    "    Y_prediction -- a numpy array (vector) containing all predictions (0/1) for the examples in X\n",
    "    '''\n",
    "    m = X.shape[1] # 获取测试集维度\n",
    "    Y_prediction = np.zeros((1,m)) # 格式化\n",
    "    w = w.reshape(X.shape[0],10) # 格式化\n",
    "    A = sigmoid(np.dot(w.T,X)+b)\n",
    "    Y_prediction = A.argmax(axis=0)\n",
    "    return Y_prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model(X_train, Y_train, X_test, Y_test, num_iterations=2000, learning_rate=0.5, print_cost=False):\n",
    "    \"\"\"\n",
    "    Builds the logistic regression model by calling the function you've implemented previously\n",
    "    \n",
    "    Arguments:\n",
    "    X_train -- training set represented by a numpy array of shape (num_px * num_px * 3, m_train)\n",
    "    Y_train -- training labels represented by a numpy array (vector) of shape (1, m_train)\n",
    "    X_test -- test set represented by a numpy array of shape (num_px * num_px * 3, m_test)\n",
    "    Y_test -- test labels represented by a numpy array (vector) of shape (1, m_test)\n",
    "    num_iterations -- hyperparameter representing the number of iterations to optimize the parameters\n",
    "    learning_rate -- hyperparameter representing the learning rate used in the update rule of optimize()\n",
    "    print_cost -- Set to true to print the cost every 100 iterations\n",
    "    \n",
    "    Returns:\n",
    "    d -- dictionary containing information about the model.\n",
    "    \"\"\"\n",
    "    \"\"\"\n",
    "    函数列表\n",
    "    sigmoid(z)    # 预测函数\n",
    "    initialize_with_zeros(dim) # 初始化wb,dim表示维度\n",
    "    propagate(w,b,X,Y) # 计算梯度以及花费\n",
    "    optimize(w, b, X, Y, num_iterations, learning_rate, print_cost = False) # 训练模型，计算新的w，新的b，以及cost，以及梯度\n",
    "    predict(w,b,X)   # 对于给定的w,b，X预测结果\n",
    "    \"\"\"\n",
    "    \n",
    "    # 初始化w，b\n",
    "    w, b = initialize_with_zeros(X_train.shape[0])\n",
    "    params,grads,costs = optimize(w,b,X_train,Y_train,num_iterations,learning_rate,print_cost)\n",
    "    w = params[\"w\"]\n",
    "    b = params[\"b\"]\n",
    "    Y_prediction_test = predict(w, b, X_test)\n",
    "    Y_prediction_train = predict(w, b, X_train)\n",
    "    print(\"训练的准确度: {} %\".format(100 - np.mean(np.abs(Y_prediction_train - Y_train)) * 100)) # mean函数求的是均值\n",
    "    print(\"测试集的准确度: {} %\".format(100 - np.mean(np.abs(Y_prediction_test - Y_test)) * 100)) # abs函数求的是绝对值\n",
    "    d = {\"costs\": costs,\n",
    "     \"Y_prediction_test\": Y_prediction_test, \n",
    "     \"Y_prediction_train\" : Y_prediction_train, \n",
    "     \"w\" : w, \n",
    "     \"b\" : b,\n",
    "     \"learning_rate\" : learning_rate,\n",
    "     \"num_iterations\": num_iterations}\n",
    "\n",
    "    return d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'train_x' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-6-0c07d38e7b77>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0md\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrain_x\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtest_x\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_test\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnum_iterations\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m1000\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlearning_rate\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m0.005\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mprint_cost\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m: name 'train_x' is not defined"
     ]
    }
   ],
   "source": [
    "d = model(train_x, y_train, test_x, y_test, num_iterations = 1000, learning_rate = 0.005, print_cost = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = np.array([[1,2,3],[4,5,6]])\n",
    "b = a.argmax(axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1 2 3]\n",
      " [4 5 6]]\n"
     ]
    }
   ],
   "source": [
    "print(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [],
   "source": [
    "a= np.random.rand(10,10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.39600368 0.84347366 0.62289595 0.5618709  0.98661532 0.63754489\n",
      "  0.55238793 0.33401225 0.10923667 0.24618096]\n",
      " [0.70154651 0.09156827 0.72991852 0.27246202 0.26224959 0.53906137\n",
      "  0.58822271 0.7720552  0.62104582 0.98254183]\n",
      " [0.72531089 0.93672468 0.98433506 0.53631866 0.21788901 0.83416178\n",
      "  0.80977391 0.47646488 0.97076137 0.66221882]\n",
      " [0.71180221 0.24269721 0.70614813 0.99139065 0.86367879 0.54892955\n",
      "  0.00997378 0.69494307 0.14245619 0.24100886]\n",
      " [0.04108674 0.33327057 0.5912546  0.2977507  0.34074109 0.95195022\n",
      "  0.29496875 0.0995444  0.90088281 0.04608654]\n",
      " [0.52494651 0.22240755 0.26069688 0.00732584 0.82919308 0.72855013\n",
      "  0.30778483 0.32857308 0.11905817 0.41250672]\n",
      " [0.96510069 0.77473835 0.16965778 0.65695264 0.17937082 0.92216955\n",
      "  0.59483597 0.67953703 0.07047638 0.38222556]\n",
      " [0.22352957 0.09723651 0.92648403 0.98163355 0.91293277 0.63397038\n",
      "  0.2459392  0.99634298 0.39182605 0.5338479 ]\n",
      " [0.14668381 0.62584329 0.19156778 0.9711091  0.93356193 0.95963238\n",
      "  0.42008057 0.98454708 0.41603376 0.18559377]\n",
      " [0.61923618 0.74738642 0.76281084 0.8202751  0.97734    0.52596194\n",
      "  0.93890369 0.95976608 0.96399078 0.17653041]]\n"
     ]
    }
   ],
   "source": [
    "print(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
